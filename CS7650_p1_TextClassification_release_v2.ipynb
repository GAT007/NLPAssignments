{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GAT007/NLPAssignments/blob/main/CS7650_p1_TextClassification_release_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj21IyLWKoxk"
      },
      "source": [
        "# Licensing Information:  You are free to use or extend this project for\n",
        "# educational purposes provided that (1) you do not distribute or publish\n",
        "# solutions, (2) you retain this notice, and (3) you provide clear\n",
        "# attribution to The Georgia Institute of Technology, including a link to https://aritter.github.io/CS-7650/\n",
        "\n",
        "# Attribution Information: This assignment was developed at The Georgia Institute of Technology\n",
        "# by Alan Ritter (alan.ritter@cc.gatech.edu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuoWLCBPu5-W"
      },
      "source": [
        "#Project \\#1: Text Classification\n",
        "\n",
        "In this assignment, you will implement the perceptron algorithm, and a simple, but competitive neural bag-of-words model, as described in [this paper](https://www.aclweb.org/anthology/P15-1162.pdf) for text classification.  You will train your models on a (provided) dataset of positive and negative movie reviews and report accuracy on a test set.\n",
        "\n",
        "In this notebook, we provide you with starter code to read in the data and evaluate the performance of your models.  After completing the instructions below, please follow the instructions at the end to submit your notebook and other files to Gradescope.\n",
        "\n",
        "Make sure to make a copy of this notebook, so your changes are saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8ZlxhBdyX8j"
      },
      "source": [
        "#Download the dataset\n",
        "\n",
        "First you will need to download the IMDB dataset - to do this, simply run the cell below.  We have prepared a small version of the ACL IMDB dataset for you to use to help make your experiments faster.  The full dataset is available [here](https://ai.stanford.edu/~amaas/data/sentiment/), in case you are interested, but there is no need to use this for the assignment.\n",
        "\n",
        "Note that files downloaded in Colab are only saved temporariliy - if your session reconnects you will need to re-download the files.  In case you need persistent storage, you can mount your Google drive folder like so:\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "You can also open a command line prompt by clicking on the shell icon on the left hand side of the page, and upload/download files from your local machine by clicking on the file icon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9r9cSojEVnB"
      },
      "source": [
        "#Download the data\n",
        "\n",
        "!wget -O aclImdb_small.tgz https://github.com/aritter/CS-7650-sp22/blob/master/slides/aclImdb_small.tgz?raw=true\n",
        "!tar xvzf aclImdb_small.tgz > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HHaUKE61udd"
      },
      "source": [
        "# Converting text to numbers\n",
        "\n",
        "Below is some code we are providing you to read in the IMDB dataset, perform tokenization (using `nltk`), and convert words into indices.  Please don't modify this code in your submitted version.  We will provide example usage below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdnNfu4uH1yj"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "import torch\n",
        "\n",
        "#Sparse matrix implementation\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, vocabFile=None):\n",
        "        self.locked = False\n",
        "        self.nextId = 0\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "        if vocabFile:\n",
        "            for line in open(vocabFile):\n",
        "                line = line.rstrip('\\n')\n",
        "                (word, wid) = line.split('\\t')\n",
        "                self.word2id[word] = int(wid)\n",
        "                self.id2word[wid] = word\n",
        "                self.nextId = max(self.nextId, int(wid) + 1)\n",
        "\n",
        "    def GetID(self, word):\n",
        "        if not word in self.word2id:\n",
        "            if self.locked:\n",
        "                return -1        #UNK token is -1.\n",
        "            else:\n",
        "                self.word2id[word] = self.nextId\n",
        "                self.id2word[self.word2id[word]] = word\n",
        "                self.nextId += 1\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def HasWord(self, word):\n",
        "        return self.word2id.has_key(word)\n",
        "\n",
        "    def HasId(self, wid):\n",
        "        return self.id2word.has_key(wid)\n",
        "\n",
        "    def GetWord(self, wid):\n",
        "        return self.id2word[wid]\n",
        "\n",
        "    def SaveVocab(self, vocabFile):\n",
        "        fOut = open(vocabFile, 'w')\n",
        "        for word in self.word2id.keys():\n",
        "            fOut.write(\"%s\\t%s\\n\" % (word, self.word2id[word]))\n",
        "\n",
        "    def GetVocabSize(self):\n",
        "        #return self.nextId-1\n",
        "        return self.nextId\n",
        "\n",
        "    def GetWords(self):\n",
        "        return self.word2id.keys()\n",
        "\n",
        "    def Lock(self):\n",
        "        self.locked = True\n",
        "\n",
        "class IMDBdata:\n",
        "    def __init__(self, directory, vocab=None):\n",
        "        \"\"\" Reads in data into sparse matrix format \"\"\"\n",
        "        pFiles = os.listdir(\"%s/pos\" % directory)\n",
        "        nFiles = os.listdir(\"%s/neg\" % directory)\n",
        "\n",
        "        if not vocab:\n",
        "            self.vocab = Vocab()\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        #For csr_matrix (see http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)\n",
        "        X_values = []\n",
        "        X_row_indices = []\n",
        "        X_col_indices = []\n",
        "        Y = []\n",
        "\n",
        "        XwordList = []\n",
        "        XfileList = []\n",
        "\n",
        "        #Read positive files\n",
        "        for i in range(len(pFiles)):\n",
        "            f = pFiles[i]\n",
        "            for line in open(\"%s/pos/%s\" % (directory, f)):\n",
        "                wordList   = [self.vocab.GetID(w.lower()) for w in word_tokenize(line) if self.vocab.GetID(w.lower()) >= 0]\n",
        "                XwordList.append(wordList)\n",
        "                XfileList.append(f)\n",
        "                wordCounts = Counter(wordList)\n",
        "                for (wordId, count) in wordCounts.items():\n",
        "                    if wordId >= 0:\n",
        "                        X_row_indices.append(i)\n",
        "                        X_col_indices.append(wordId)\n",
        "                        X_values.append(count)\n",
        "            Y.append(+1.0)\n",
        "\n",
        "        #Read negative files\n",
        "        for i in range(len(nFiles)):\n",
        "            f = nFiles[i]\n",
        "            for line in open(\"%s/neg/%s\" % (directory, f)):\n",
        "                wordList   = [self.vocab.GetID(w.lower()) for w in word_tokenize(line) if self.vocab.GetID(w.lower()) >= 0]\n",
        "                XwordList.append(wordList)\n",
        "                XfileList.append(f)\n",
        "                wordCounts = Counter(wordList)\n",
        "                for (wordId, count) in wordCounts.items():\n",
        "                    if wordId >= 0:\n",
        "                        X_row_indices.append(len(pFiles)+i)\n",
        "                        X_col_indices.append(wordId)\n",
        "                        X_values.append(count)\n",
        "            Y.append(-1.0)\n",
        "\n",
        "        self.vocab.Lock()\n",
        "\n",
        "        #Create a sparse matrix in csr format\n",
        "        self.X = csr_matrix((X_values, (X_row_indices, X_col_indices)), shape=(max(X_row_indices)+1, self.vocab.GetVocabSize()))\n",
        "        self.Y = np.asarray(Y)\n",
        "\n",
        "        #Randomly shuffle\n",
        "        index = np.arange(self.X.shape[0])\n",
        "        np.random.shuffle(index)\n",
        "        self.X = self.X[index,:]\n",
        "        self.XwordList = [torch.LongTensor(XwordList[i]) for i in index]  #Two different sparse formats, csr and lists of IDs (XwordList).\n",
        "        self.XfileList = [XfileList[i] for i in index]\n",
        "        self.Y = self.Y[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn1gvbW92h1V"
      },
      "source": [
        "The code below reads the train,dev and test data into memory.  This will take a minute or so.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGp6dMNZEGoP"
      },
      "source": [
        "train = IMDBdata(\"aclImdb_small/train\")\n",
        "train.vocab.Lock()\n",
        "dev  = IMDBdata(\"aclImdb_small/dev\", vocab=train.vocab)\n",
        "test  = IMDBdata(\"aclImdb_small/test\", vocab=train.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1pnYB3I22QJ"
      },
      "source": [
        "# Exploring the data\n",
        "\n",
        "Below are a few examples to help you get familiar with how the data is represented.  $X \\in \\mathbb{R}^{N \\times M}$ contains the design matrix and $Y \\in \\{+1,-1\\}^N$ contains the labels.  Because there are a large number of features in this representation ($X$ contains one column representing each unique word in the dataset), we are using a sparse matrix representation (Numpy's [csr_sparse](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)).  PyTorch doesn't have great support for sparse matrices...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idqR_DvV3USp"
      },
      "source": [
        "# X contains the design matrix representing the training data.\n",
        "print(f\"Train.X has {train.X.shape[0]} rows and {train.X.shape[1]} columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY_jAvEl8O4V"
      },
      "source": [
        "# Labels\n",
        "train.Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5w0c8WA8ixA"
      },
      "source": [
        "# Let's count the frequency of every word appearing in the documents with negative sentiment:\n",
        "word_counts = np.array(train.X[train.Y == -1,:].sum(axis=0)).flatten()\n",
        "word_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE67XmOn85lC"
      },
      "source": [
        "# Now, let's sort the words by frequency:\n",
        "sorted_words = list(reversed(np.argsort(word_counts)))\n",
        "sorted_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-YA01vB9IPw"
      },
      "source": [
        "# What is the index of the most frequent word?\n",
        "sorted_words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWIT_E1N9P-X"
      },
      "source": [
        "# Let's see what word that is:\n",
        "train.vocab.GetWord(sorted_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHTtRxDR9THA"
      },
      "source": [
        "# What are the 10 most frequent words?\n",
        "[train.vocab.GetWord(sorted_words[x]) for x in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtY58U5Y9rNT"
      },
      "source": [
        "# Now it's time to write some code!\n",
        "\n",
        "# Basic Perceptron (10 points)\n",
        "\n",
        "Implement the perceptron classification algorithm (fill in the missing code marked with `TODO:` below).\n",
        "The only hyperparameter to tune is the number of iterations.\n",
        "Accuracy > 80% is acceptable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_ZlositHwng"
      },
      "source": [
        "class Eval:\n",
        "    def __init__(self, pred, gold):\n",
        "        self.pred = pred\n",
        "        self.gold = gold\n",
        "\n",
        "    def Accuracy(self):\n",
        "        return np.sum(np.equal(self.pred, self.gold)) / float(len(self.gold))\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, X, Y, N_ITERATIONS):\n",
        "        #TODO: Initalize parameters\n",
        "        self.Train(X,Y)\n",
        "\n",
        "    def ComputeAverageParameters(self):\n",
        "        #TODO: Compute average parameters (do this part last)\n",
        "        return\n",
        "\n",
        "    def Train(self, X, Y):\n",
        "        #TODO: Estimate perceptron parameters\n",
        "        return\n",
        "\n",
        "    def Predict(self, X):\n",
        "        #TODO: Implement perceptron classification\n",
        "        return [1 for i in range(X.shape[0])]\n",
        "\n",
        "    def SavePredictions(self, data, outFile):\n",
        "        Y_pred = self.Predict(data.X)\n",
        "        fOut = open(outFile, 'w')\n",
        "        for i in range(len(data.XfileList)):\n",
        "          fOut.write(f\"{data.XfileList[i]}\\t{Y_pred[i]}\\n\")\n",
        "\n",
        "    def Eval(self, X_test, Y_test):\n",
        "        Y_pred = self.Predict(X_test)\n",
        "        ev = Eval(Y_pred, Y_test)\n",
        "        return ev.Accuracy()\n",
        "\n",
        "ptron = Perceptron(train.X, train.Y, 10)\n",
        "ptron.ComputeAverageParameters()\n",
        "print(ptron.Eval(dev.X, dev.Y))\n",
        "\n",
        "ptron.SavePredictions(test, 'test_pred_perceptron.txt')\n",
        "\n",
        "print(train.X.shape)\n",
        "print(test.X.shape)\n",
        "\n",
        "#TODO: Print the 20 most positive and 20 most negative words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rhcI8_MsfPF"
      },
      "source": [
        "# Parameter Averaging (5 points)\n",
        "\n",
        "Once your basic perceptron implementation is working, modify code to implement parameter averaging.  Instead of using parameters from the final iteration, $w_n$, to classify test examples,\n",
        "use the average of the parameters from every iteration, $\\frac{1}{N}\\sum_{i=1}^N w_i$.  A nice trick for doing this efficiently is described in section 2.1.1 of [Hal Daume's thesis](https://github.com/aritter/CS-7650-sp25/blob/master/slides/document%20(1).pdf?raw=true).  When you are done, save a copy of your predictions on the test set to turn in at the end (click on the file icon on the left side of the notebook).\n",
        "For this part, accuracy > 82% is acceptable.\n",
        "\n",
        "# Inspect Feature Weights (5 points)\n",
        "\n",
        "Modify the code block above to print out the 20 most positive and 20 most negative words in the vocabulary sorted by their learned weights according to your model.  This will require a bit of thought how to do because the words in each document have been converted to IDs (see examples above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we_vLQ5YRkJv"
      },
      "source": [
        "# Evaluate on the test set\n",
        "\n",
        "Once you are happy with your performance on the dev set, run the cell below to evaluate your performance on the test set.  Make sure to download the predictions of your model in `test_pred_perceptron.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY9wVRHPRkJ8"
      },
      "source": [
        "print(ptron.Eval(test.X, test.Y))\n",
        "\n",
        "ptron.SavePredictions(test, 'test_pred_perceptron.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2dykgBhuVvr"
      },
      "source": [
        "# PyTorch Introduction\n",
        "\n",
        "Take a look at the code below, which provides an example of how a simple neural network is capable of learning the XOR function (note that the perceptron cannot do this).  Refer to the PyTorch documentation [here](https://pytorch.org/docs/stable/index.html) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18KP4ktWIQfy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "#Define the computation graph; one layer hidden network\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, dim_i, dim_h, dim_o):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.V = nn.Linear(dim_i, dim_h)\n",
        "        self.g = nn.Tanh()\n",
        "        self.W = nn.Linear(dim_h, dim_o)\n",
        "        self.logSoftmax = nn.LogSoftmax(dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.logSoftmax(self.W(self.g(self.V(x))))\n",
        "\n",
        "train_X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "train_Y = np.array([0,1,1,0])\n",
        "\n",
        "num_classes  = 2\n",
        "num_hidden   = 10\n",
        "num_features = 2\n",
        "\n",
        "ffnn = FFNN(num_features, num_hidden, num_classes)\n",
        "optimizer = optim.Adam(ffnn.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0.0\n",
        "    #Randomly shuffle examples in each epoch\n",
        "    shuffled_i = list(range(0,len(train_Y)))\n",
        "    random.shuffle(shuffled_i)\n",
        "    for i in shuffled_i:\n",
        "        x        = torch.from_numpy(train_X[i]).float()\n",
        "        y_onehot = torch.zeros(num_classes)\n",
        "        y_onehot[train_Y[i]] = 1\n",
        "\n",
        "        ffnn.zero_grad()\n",
        "        logProbs = ffnn.forward(x)\n",
        "        loss = torch.neg(logProbs).dot(y_onehot)\n",
        "        total_loss += loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "      print(\"loss on epoch %i: %f\" % (epoch, total_loss))\n",
        "\n",
        "#Evaluate on the training set:\n",
        "num_errors = 0\n",
        "for i in range(len(train_Y)):\n",
        "    x = torch.from_numpy(train_X[i]).float()\n",
        "    y = train_Y[i]\n",
        "    logProbs = ffnn.forward(x)\n",
        "    prediction = torch.argmax(logProbs)\n",
        "    if y != prediction:\n",
        "        num_errors += 1\n",
        "print(\"number of errors: %d\" % num_errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4QDzv6OwlSO"
      },
      "source": [
        "# Neural Bag-of-Words (10 points)\n",
        "\n",
        "Your next task is to implement a neural bag-of-words baseline, NBOW-RAND, as described in [this paper](https://www.aclweb.org/anthology/P15-1162.pdf).  See section 2.1.  Note that the dataset we are using is a sample of the full IMDB dataset to make developing your solution easier, so your performance will be a bit lower than the numbers reported in the paper for the full dataset.\n",
        "\n",
        "# GPUs\n",
        "\n",
        "You probably want to use a GPU to enable faster training with larger word embeddings and hidden layers (the paper listed above uses 300 dimensions).\n",
        "\n",
        "Colab provides free access to GPUs, which will be useful for rest of the assignment.  To access a GPU, select `Runtime` from the menu at the top of the page, and then choose `Change Runtime Type`; under the `Hardware Accelerator` dropdown select `GPU`.  Note that the free version of Colab does have quotas on how much GPU can be used - you won't need to use a GPU for the first part of the assignment (implementing the Perceptron algorithm).\n",
        "\n",
        "You can check to make sure a GPU is available using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLOCwLE5HEq0"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCiewAPLG6lK"
      },
      "source": [
        "# Data format\n",
        "\n",
        "In addition to reading in the data in sparse matrix (Numpy CSR) format, which you used in your implementation of the perceptron algorithm, the data loading code above also reads it in another format in `train.XwordList`.  This contains a list of PyTorch arrays consisting of sequences of word IDs corresponding to the content of each document.  The documents in `train.XwordList` are presented in the same order as labels (`train.Y`).  For the next part of the assignment, you should work with the data in this new format, which is fairly standard for text data in neural networks.  See below for a few examples on how to access the data in this new format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORUdEdld2ZLm"
      },
      "source": [
        "# Example of using XwordList\n",
        "\n",
        "print(\"Here is the first document:\")\n",
        "print(train.XwordList[0].tolist())\n",
        "\n",
        "print(\"After converting IDs to words:\")\n",
        "print([train.vocab.GetWord(x) for x in train.XwordList[0].tolist()])\n",
        "\n",
        "print(\"Here is the label:\")\n",
        "print(train.Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK2ImoAFBL7V"
      },
      "source": [
        "# Suggestions for getting started\n",
        "\n",
        "We recommend following a similar structure as the XOR example above, starting with an [Embedding Layer](https://pytorch.org/docs/stable/nn.html\\#embedding), a single hidden layer and [Adam optimizer.](https://pytorch.org/docs/stable/optim.html\\#torch.optim.Adam)\n",
        "Please refer to the Pytorch Documentation for more information as necessary.  You can use a batch size of one for this assignment, to simplify your implementation.\n",
        "Feel free to change the number of iterations and learning rate. For this part, accuracy > 83% is acceptable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex-40V7SMd6L"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "    def __init__(self, VOCAB_SIZE, DIM_EMB=300, NUM_CLASSES=2):\n",
        "        super(NBOW, self).__init__()\n",
        "        self.NUM_CLASSES=NUM_CLASSES\n",
        "        #TODO: Initialize parameters.\n",
        "\n",
        "    def forward(self, X):\n",
        "        #TODO: Implement forward computation.\n",
        "        return torch.randn(self.NUM_CLASSES)\n",
        "\n",
        "def EvalNet(data, net):\n",
        "    num_correct = 0\n",
        "    Y = (data.Y + 1.0) / 2.0\n",
        "    X = data.XwordList\n",
        "    for i in range(len(X)):\n",
        "        logProbs = net.forward(X[i])\n",
        "        pred = torch.argmax(logProbs)\n",
        "        if pred == Y[i]:\n",
        "            num_correct += 1\n",
        "    print(\"Accuracy: %s\" % (float(num_correct) / float(len(X))))\n",
        "\n",
        "def SavePredictions(data, outFile, net):\n",
        "    fOut = open(outFile, 'w')\n",
        "    for i in range(len(data.XwordList)):\n",
        "        logProbs = net.forward(data.XwordList[i])\n",
        "        pred = torch.argmax(logProbs)\n",
        "        fOut.write(f\"{data.XfileList[i]}\\t{pred}\\n\")\n",
        "\n",
        "def Train(net, X, Y, n_iter, dev):\n",
        "    print(\"Start Training!\")\n",
        "    #TODO: initialize optimizer.\n",
        "\n",
        "    num_classes = len(set(Y))\n",
        "\n",
        "    for epoch in range(n_iter):\n",
        "        num_correct = 0\n",
        "        total_loss = 0.0\n",
        "        net.train()   #Put the network into training mode\n",
        "        for i in tqdm(range(len(X))):\n",
        "            pass\n",
        "            #TODO: compute gradients, do parameter update, compute loss.\n",
        "        net.eval()    #Switch to eval mode\n",
        "        print(f\"loss on epoch {epoch} = {total_loss}\")\n",
        "        EvalNet(dev, net)\n",
        "\n",
        "nbow = NBOW(train.vocab.GetVocabSize()).cuda()\n",
        "Train(nbow, train.XwordList, (train.Y + 1.0) / 2.0, 5, dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fka0q0f2QfVT"
      },
      "source": [
        "# Evaluate on the test set\n",
        "\n",
        "Once you are happy with your performance on the dev set, run the cell below to evaluate your performance on the test set.  Make sure to download the predictions of your model in `test_pred_nbow.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5N7dFX5Q3_R"
      },
      "source": [
        "EvalNet(test, nbow)\n",
        "SavePredictions(test, 'test_pred_nbow.txt', nbow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLry9u8kCAQM"
      },
      "source": [
        "# 1-D Convolutional Neural Networks (2 points - optional extra credit)\n",
        "\n",
        "The final task for this assignment is to implement a convolutional neural network for text classification similar to the CNN-rand baseline described by [Kim (2014)](https://www.aclweb.org/anthology/D14-1181.pdf).  We haven't covered CNNs in lecture yet, so this part is optional.  You should use the same data format as the NBOW model above, starting out with an [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer, followed by a [1-D convolution](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html), a [max-pooling layer](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d) and a [Dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).  You should be able to use the same `Train()` function you wrote above. Accuracy > 83% is acceptable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iY3QTL6Npdy"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, VOCAB_SIZE, DIM_EMB=300, NUM_CLASSES=2):\n",
        "        super(CNN, self).__init__()\n",
        "        self.NUM_CLASSES=NUM_CLASSES\n",
        "        #TODO: Initialize parameters.\n",
        "\n",
        "    def forward(self, X):\n",
        "        #TODO: Implement forward computation.\n",
        "        return torch.randn(self.NUM_CLASSES)\n",
        "\n",
        "cnn = CNN(train.vocab.GetVocabSize()).cuda()\n",
        "Train(cnn, train.XwordList, (train.Y + 1.0) / 2.0, 5, dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NB1kC0fTCWt"
      },
      "source": [
        "# Evaluate on the test set\n",
        "\n",
        "Once you are happy with your performance on the dev set, run the cell below to evaluate your performance on the test set.  Make sure to download the predictions of your model in `test_pred_nbow.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyrQ1Z76TCW9"
      },
      "source": [
        "EvalNet(test, cnn)\n",
        "SavePredictions(test, 'test_pred_cnn.txt', cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcKYByOTDwKN"
      },
      "source": [
        "## Gradescope\n",
        "\n",
        "Gradescope allows you to add multiple files to your submission. Please submit this notebook along with the test set prediction:\n",
        "* test_pred_perceptron.txt\n",
        "* test_pred_nbow.txt\n",
        "* test_pred_cnn.txt (optional)\n",
        "* TextClassification_release.ipynb\n",
        "\n",
        "To download this notebook, go to `File > Download.ipynb`. You can download the predictions from Colab by clicking the folder icon on the left and finding them under Files.\n",
        "\n",
        "Please make sure that you name the files as specified above. You will be able to see the test set accuracy for your predictions. However, the final score will be assigned later based on accuracy and implementation.\n",
        "\n",
        "When submitting the .ipynb notebook, please make sure that all the cells run when executed in order starting from a fresh session. If the code doesn't take too long to run, you can re-run everything with `Runtime -> Restart and run all`\n",
        "\n",
        "You can submit multiple times before the deadline and choose the submission which you want to be graded by going to `Submission History` on gradescope.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll5P0dB-ny94"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}